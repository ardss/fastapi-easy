# FastAPI-Easy æ€§èƒ½ä¼˜åŒ–å®æ–½æŒ‡å—

**ç‰ˆæœ¬**: 1.0  
**æ—¥æœŸ**: 2025-11-28  
**ä½œè€…**: Performance Team

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [ä¼˜åŒ–æ¨¡å—ä»‹ç»](#ä¼˜åŒ–æ¨¡å—ä»‹ç»)
3. [å®æ–½æ­¥éª¤](#å®æ–½æ­¥éª¤)
4. [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å¯ç”¨æŸ¥è¯¢ç¼“å­˜

```python
from fastapi_easy.core.cache import get_query_cache

# è·å–å…¨å±€ç¼“å­˜å®ä¾‹
cache = get_query_cache()

# ç¼“å­˜æŸ¥è¯¢ç»“æœ
cache_key = cache._generate_key("users", id=1)
await cache.set(cache_key, user_data, ttl=300)

# è¯»å–ç¼“å­˜
cached_user = await cache.get(cache_key)
```

### ä½¿ç”¨æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
from fastapi_easy.core.batch import create_bulk_insert_optimizer

# åˆ›å»ºæ‰¹é‡æ’å…¥ä¼˜åŒ–å™¨
optimizer = create_bulk_insert_optimizer(batch_size=100)

# æ‰¹é‡æ’å…¥ 1000 æ¡è®°å½•
items = [{"name": f"item_{i}", "value": i} for i in range(1000)]
results = await optimizer.bulk_insert(items, adapter.create)
```

### é…ç½®è¿æ¥æ± 

```python
from fastapi_easy.core.pool import production_pool_config

# è·å–ç”Ÿäº§ç¯å¢ƒé…ç½®
pool_config = production_pool_config()

# æˆ–ä½¿ç”¨æ„å»ºå™¨è‡ªå®šä¹‰é…ç½®
from fastapi_easy.core.pool import PoolConfigBuilder

config = (
    PoolConfigBuilder()
    .with_pool_size(50)
    .with_max_overflow(20)
    .with_pool_recycle(7200)
    .build()
)
```

---

## ğŸ“š ä¼˜åŒ–æ¨¡å—ä»‹ç»

### 1. æŸ¥è¯¢ç¼“å­˜æ¨¡å— (`fastapi_easy.core.cache`)

**åŠŸèƒ½**: ä½¿ç”¨ TTL æ”¯æŒçš„å†…å­˜ç¼“å­˜å­˜å‚¨æŸ¥è¯¢ç»“æœ

**ç‰¹æ€§**:
- âœ… è‡ªåŠ¨ TTL è¿‡æœŸç®¡ç†
- âœ… æœ€å¤§å®¹é‡é™åˆ¶ï¼ˆLRU æ·˜æ±°ï¼‰
- âœ… å¼‚æ­¥å®‰å…¨ï¼ˆä½¿ç”¨ asyncio.Lockï¼‰
- âœ… è‡ªåŠ¨è¿‡æœŸæ¸…ç†

**æ€§èƒ½æå‡**: 50-70% æ•°æ®åº“æŸ¥è¯¢å‡å°‘

**ä½¿ç”¨åœºæ™¯**:
- çƒ­æ•°æ®ç¼“å­˜ï¼ˆç”¨æˆ·ä¿¡æ¯ã€é…ç½®ç­‰ï¼‰
- é¢‘ç¹æŸ¥è¯¢çš„ç»“æœ
- è¯»å¤šå†™å°‘çš„æ•°æ®

**ç¤ºä¾‹**:

```python
from fastapi_easy.core.cache import QueryCache

# åˆ›å»ºç¼“å­˜å®ä¾‹
cache = QueryCache(max_size=1000, default_ttl=300)

# ç¼“å­˜ç”¨æˆ·æŸ¥è¯¢ç»“æœ
user_key = cache._generate_key("user", id=123)
await cache.set(user_key, user_data)

# è¯»å–ç¼“å­˜
cached_user = await cache.get(user_key)

# æ¸…ç†è¿‡æœŸæ•°æ®
removed = await cache.cleanup_expired()
print(f"Removed {removed} expired entries")

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
stats = cache.get_stats()
print(f"Cache usage: {stats['usage_percent']:.1f}%")
```

### 2. æ‰¹é‡æ“ä½œæ¨¡å— (`fastapi_easy.core.batch`)

**åŠŸèƒ½**: ä¼˜åŒ–æ‰¹é‡ CRUD æ“ä½œçš„æ€§èƒ½

**ç‰¹æ€§**:
- âœ… è‡ªåŠ¨åˆ†æ‰¹å¤„ç†
- âœ… å¹¶å‘æ‰§è¡Œæ”¯æŒ
- âœ… çµæ´»çš„æ‰¹å¤§å°é…ç½®
- âœ… é”™è¯¯å¤„ç†

**æ€§èƒ½æå‡**: 5-10 å€ï¼ˆæ‰¹é‡æ’å…¥ï¼‰

**ä½¿ç”¨åœºæ™¯**:
- æ‰¹é‡å¯¼å…¥æ•°æ®
- æ‰¹é‡æ›´æ–°æ“ä½œ
- æ‰¹é‡åˆ é™¤æ“ä½œ

**ç¤ºä¾‹**:

```python
from fastapi_easy.core.batch import (
    create_batch_processor,
    create_bulk_insert_optimizer,
    create_bulk_update_optimizer,
    create_bulk_delete_optimizer,
)

# æ‰¹é‡æ’å…¥
insert_optimizer = create_bulk_insert_optimizer(batch_size=100)
items = [{"name": f"item_{i}"} for i in range(1000)]
results = await insert_optimizer.bulk_insert(items, adapter.create)

# æ‰¹é‡æ›´æ–°
update_optimizer = create_bulk_update_optimizer(batch_size=100)
updates = [{"id": i, "status": "active"} for i in range(1000)]
updated_count = await update_optimizer.bulk_update(updates, adapter.update)

# æ‰¹é‡åˆ é™¤
delete_optimizer = create_bulk_delete_optimizer(batch_size=100)
ids = list(range(1000))
deleted_count = await delete_optimizer.bulk_delete(ids, adapter.delete_one)
```

### 3. è¿æ¥æ± é…ç½®æ¨¡å— (`fastapi_easy.core.pool`)

**åŠŸèƒ½**: ç®¡ç†æ•°æ®åº“è¿æ¥æ± é…ç½®

**ç‰¹æ€§**:
- âœ… é¢„å®šä¹‰ç¯å¢ƒé…ç½®ï¼ˆå¼€å‘ã€ç”Ÿäº§ã€é«˜æ€§èƒ½ï¼‰
- âœ… çµæ´»çš„æ„å»ºå™¨ API
- âœ… è¿æ¥ç”Ÿå‘½å‘¨æœŸç®¡ç†
- âœ… æ€§èƒ½ç›‘æ§æ”¯æŒ

**æ€§èƒ½æå‡**: 30% è¿æ¥å¼€é”€å‡å°‘

**ä½¿ç”¨åœºæ™¯**:
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- é«˜å¹¶å‘åº”ç”¨
- é•¿è¿æ¥ç®¡ç†

**ç¤ºä¾‹**:

```python
from fastapi_easy.core.pool import (
    development_pool_config,
    production_pool_config,
    high_performance_pool_config,
    PoolConfigBuilder,
)

# ä½¿ç”¨é¢„å®šä¹‰é…ç½®
dev_config = development_pool_config()
prod_config = production_pool_config()
perf_config = high_performance_pool_config()

# è‡ªå®šä¹‰é…ç½®
custom_config = (
    PoolConfigBuilder()
    .with_pool_size(100)
    .with_max_overflow(50)
    .with_pool_timeout(60)
    .with_pool_recycle(7200)
    .with_pool_pre_ping(True)
    .build()
)

# è½¬æ¢ä¸ºå­—å…¸ç”¨äºæ•°æ®åº“å¼•æ“
config_dict = custom_config.to_dict()
```

---

## ğŸ”§ å®æ–½æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ä¼˜åŒ–ï¼ˆ1-2 å‘¨ï¼‰

#### æ­¥éª¤ 1: å¯ç”¨æŸ¥è¯¢ç¼“å­˜

```python
# åœ¨åº”ç”¨åˆå§‹åŒ–æ—¶
from fastapi_easy.core.cache import create_query_cache

# åˆ›å»ºç¼“å­˜å®ä¾‹
query_cache = create_query_cache(max_size=5000, default_ttl=600)

# åœ¨é€‚é…å™¨ä¸­ä½¿ç”¨ç¼“å­˜
class CachedAdapter:
    def __init__(self, adapter, cache):
        self.adapter = adapter
        self.cache = cache
    
    async def get_all(self, filters, sorts, pagination):
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.cache._generate_key(
            "get_all",
            filters=str(filters),
            sorts=str(sorts),
            pagination=str(pagination)
        )
        
        # æ£€æŸ¥ç¼“å­˜
        cached = await self.cache.get(cache_key)
        if cached is not None:
            return cached
        
        # æ‰§è¡ŒæŸ¥è¯¢
        result = await self.adapter.get_all(filters, sorts, pagination)
        
        # ç¼“å­˜ç»“æœ
        await self.cache.set(cache_key, result)
        return result
```

**é¢„æœŸæ•ˆæœ**: å‡å°‘ 50-70% æ•°æ®åº“æŸ¥è¯¢

#### æ­¥éª¤ 2: ä¼˜åŒ–æ‰¹é‡æ“ä½œ

```python
# åœ¨æ•°æ®å¯¼å…¥æ—¶ä½¿ç”¨æ‰¹é‡æ“ä½œ
from fastapi_easy.core.batch import create_bulk_insert_optimizer

async def import_users(file_path):
    optimizer = create_bulk_insert_optimizer(batch_size=500)
    
    # è¯»å–æ•°æ®
    users = read_csv(file_path)
    
    # æ‰¹é‡æ’å…¥
    results = await optimizer.bulk_insert(users, adapter.create)
    return len(results)
```

**é¢„æœŸæ•ˆæœ**: æå‡ 5-10 å€æ€§èƒ½

#### æ­¥éª¤ 3: é…ç½®è¿æ¥æ± 

```python
# åœ¨æ•°æ®åº“åˆå§‹åŒ–æ—¶
from fastapi_easy.core.pool import production_pool_config
from sqlalchemy.ext.asyncio import create_async_engine

pool_config = production_pool_config()
config_dict = pool_config.to_dict()

engine = create_async_engine(
    "postgresql+asyncpg://user:password@localhost/db",
    **config_dict
)
```

**é¢„æœŸæ•ˆæœ**: å‡å°‘ 30% è¿æ¥å¼€é”€

### ç¬¬äºŒé˜¶æ®µï¼šé«˜çº§ä¼˜åŒ–ï¼ˆ2-3 å‘¨ï¼‰

#### æ­¥éª¤ 4: å®ç°å¤šå±‚ç¼“å­˜

```python
class MultiLayerCache:
    def __init__(self):
        self.l1_cache = create_query_cache(max_size=1000, default_ttl=60)
        self.l2_cache = create_query_cache(max_size=10000, default_ttl=600)
    
    async def get(self, key):
        # L1: çƒ­æ•°æ®ç¼“å­˜
        result = await self.l1_cache.get(key)
        if result:
            return result
        
        # L2: å†·æ•°æ®ç¼“å­˜
        result = await self.l2_cache.get(key)
        if result:
            # æå‡åˆ° L1
            await self.l1_cache.set(key, result, ttl=60)
            return result
        
        return None
    
    async def set(self, key, value):
        await self.l1_cache.set(key, value, ttl=60)
        await self.l2_cache.set(key, value, ttl=600)
```

**é¢„æœŸæ•ˆæœ**: å‡å°‘ 70-90% æ•°æ®åº“æŸ¥è¯¢

#### æ­¥éª¤ 5: å¼‚æ­¥æ‰¹é‡æ“ä½œ

```python
from fastapi_easy.core.batch import create_batch_processor

async def process_large_dataset():
    processor = create_batch_processor(batch_size=100, max_concurrent=5)
    
    async def process_batch(batch):
        # å¹¶å‘å¤„ç†æ‰¹æ¬¡
        tasks = [process_item(item) for item in batch]
        return await asyncio.gather(*tasks)
    
    items = load_items()
    results = await processor.process_batch(items, process_batch)
```

**é¢„æœŸæ•ˆæœ**: æå‡ 3-5 å€ååé‡

### ç¬¬ä¸‰é˜¶æ®µï¼šç”Ÿäº§ä¼˜åŒ–ï¼ˆ3-4 å‘¨ï¼‰

#### æ­¥éª¤ 6: è¿ç§»åˆ° PostgreSQL

```python
# ä½¿ç”¨ PostgreSQL æ›¿ä»£ SQLite
from sqlalchemy.ext.asyncio import create_async_engine
from fastapi_easy.core.pool import high_performance_pool_config

pool_config = high_performance_pool_config()
config_dict = pool_config.to_dict()

engine = create_async_engine(
    "postgresql+asyncpg://user:password@localhost/db",
    **config_dict
)
```

**é¢„æœŸæ•ˆæœ**: æå‡ 10-50 å€æ€§èƒ½

#### æ­¥éª¤ 7: æ·»åŠ æ•°æ®åº“ç´¢å¼•

```python
# åœ¨æ¨¡å‹ä¸­æ·»åŠ ç´¢å¼•
from sqlalchemy import Index

class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), index=True)  # æ·»åŠ ç´¢å¼•
    email = Column(String(100), index=True)  # æ·»åŠ ç´¢å¼•
    created_at = Column(DateTime, index=True)  # æ·»åŠ ç´¢å¼•
    
    # å¤åˆç´¢å¼•
    __table_args__ = (
        Index('idx_user_email_status', 'email', 'status'),
    )
```

**é¢„æœŸæ•ˆæœ**: æå‡ 2-5 å€æŸ¥è¯¢é€Ÿåº¦

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### ç¼“å­˜æ•ˆæœå¯¹æ¯”

```
åœºæ™¯: æŸ¥è¯¢ 10000 æ¡è®°å½•

æ— ç¼“å­˜:
- æŸ¥è¯¢æ—¶é—´: 1000ms
- æ•°æ®åº“æŸ¥è¯¢: 100 æ¬¡
- æ€»è€—æ—¶: 100s

å¯ç”¨ç¼“å­˜:
- æŸ¥è¯¢æ—¶é—´: 1000ms (é¦–æ¬¡)
- æŸ¥è¯¢æ—¶é—´: 10ms (ç¼“å­˜å‘½ä¸­)
- æ•°æ®åº“æŸ¥è¯¢: 1 æ¬¡
- æ€»è€—æ—¶: 1s (ç¼“å­˜å‘½ä¸­ç‡ 99%)

æ€§èƒ½æå‡: 100 å€ âœ…
```

### æ‰¹é‡æ“ä½œæ•ˆæœå¯¹æ¯”

```
åœºæ™¯: æ’å…¥ 1000 æ¡è®°å½•

é€æ¡æ’å…¥:
- æ¯æ¡è€—æ—¶: 10ms
- æ€»è€—æ—¶: 10s
- æ•°æ®åº“æ“ä½œ: 1000 æ¬¡

æ‰¹é‡æ’å…¥ (batch_size=100):
- æ¯æ‰¹è€—æ—¶: 50ms
- æ€»è€—æ—¶: 500ms
- æ•°æ®åº“æ“ä½œ: 10 æ¬¡

æ€§èƒ½æå‡: 20 å€ âœ…
```

### è¿æ¥æ± æ•ˆæœå¯¹æ¯”

```
åœºæ™¯: 100 å¹¶å‘è¯·æ±‚

æ— è¿æ¥æ± :
- åˆ›å»ºè¿æ¥: 100 æ¬¡
- è¿æ¥å¼€é”€: 100 * 50ms = 5000ms
- æ€»è€—æ—¶: 5000ms

ä½¿ç”¨è¿æ¥æ± :
- åˆ›å»ºè¿æ¥: 20 æ¬¡
- è¿æ¥å¤ç”¨: 80 æ¬¡
- è¿æ¥å¼€é”€: 20 * 50ms = 1000ms
- æ€»è€—æ—¶: 1000ms

æ€§èƒ½æå‡: 5 å€ âœ…
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ç¼“å­˜ç­–ç•¥

```python
# âœ… å¥½çš„åšæ³•
# ç¼“å­˜çƒ­æ•°æ®ï¼ˆç”¨æˆ·ä¿¡æ¯ã€é…ç½®ï¼‰
cache_key = cache._generate_key("user", id=user_id)
await cache.set(cache_key, user_data, ttl=600)

# âŒ ä¸å¥½çš„åšæ³•
# ç¼“å­˜æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬å†·æ•°æ®ï¼‰
# è¿™ä¼šå¯¼è‡´ç¼“å­˜å‘½ä¸­ç‡ä½
```

### 2. æ‰¹é‡æ“ä½œ

```python
# âœ… å¥½çš„åšæ³•
# ä½¿ç”¨åˆç†çš„æ‰¹å¤§å°
optimizer = create_bulk_insert_optimizer(batch_size=100)

# âŒ ä¸å¥½çš„åšæ³•
# æ‰¹å¤§å°è¿‡å¤§å¯¼è‡´å†…å­˜æº¢å‡º
optimizer = create_bulk_insert_optimizer(batch_size=10000)
```

### 3. è¿æ¥æ± é…ç½®

```python
# âœ… å¥½çš„åšæ³•
# æ ¹æ®ç¯å¢ƒé€‰æ‹©åˆé€‚çš„é…ç½®
if env == "production":
    config = production_pool_config()
elif env == "development":
    config = development_pool_config()

# âŒ ä¸å¥½çš„åšæ³•
# ä½¿ç”¨é»˜è®¤é…ç½®ä¸é€‚åˆç”Ÿäº§ç¯å¢ƒ
```

### 4. ç›‘æ§å’Œè°ƒä¼˜

```python
# âœ… å®šæœŸç›‘æ§ç¼“å­˜ç»Ÿè®¡
stats = cache.get_stats()
if stats['usage_percent'] > 90:
    # å¢åŠ ç¼“å­˜å¤§å°æˆ–è°ƒæ•´ TTL
    pass

# âœ… ç›‘æ§æ•°æ®åº“æ€§èƒ½
# ä½¿ç”¨ SQLAlchemy echo åŠŸèƒ½
# æˆ–ä½¿ç”¨ä¸“ä¸šç›‘æ§å·¥å…·
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: ç¼“å­˜ä¼šå¯¼è‡´æ•°æ®ä¸ä¸€è‡´å—ï¼Ÿ

**A**: æ˜¯çš„ï¼Œéœ€è¦åœ¨æ›´æ–°æ•°æ®æ—¶æ¸…é™¤ç¼“å­˜ï¼š

```python
async def update_user(user_id, data):
    # æ›´æ–°æ•°æ®åº“
    result = await adapter.update(user_id, data)
    
    # æ¸…é™¤ç¼“å­˜
    cache_key = cache._generate_key("user", id=user_id)
    await cache.delete(cache_key)
    
    return result
```

### Q2: æ‰¹é‡æ“ä½œçš„æœ€ä½³æ‰¹å¤§å°æ˜¯å¤šå°‘ï¼Ÿ

**A**: æ ¹æ®æ•°æ®å¤§å°è°ƒæ•´ï¼š

```
- å°æ•°æ® (< 1KB): batch_size = 500-1000
- ä¸­ç­‰æ•°æ® (1-10KB): batch_size = 100-500
- å¤§æ•°æ® (> 10KB): batch_size = 10-100
```

### Q3: å¦‚ä½•é€‰æ‹©åˆé€‚çš„è¿æ¥æ± å¤§å°ï¼Ÿ

**A**: æ ¹æ®å¹¶å‘æ•°è®¡ç®—ï¼š

```
pool_size = (å¹¶å‘æ•° / å¹³å‡è¯·æ±‚å¤„ç†æ—¶é—´) * 1.2
max_overflow = pool_size * 0.5

ä¾‹å¦‚:
- 100 å¹¶å‘ï¼Œå¹³å‡å¤„ç† 100ms
- pool_size = (100 / 0.1) * 1.2 = 1200 (å¤ªå¤§)
- å®é™…: pool_size = 20, max_overflow = 10
```

### Q4: ç¼“å­˜è¿‡æœŸæ—¶é—´åº”è¯¥è®¾ç½®å¤šé•¿ï¼Ÿ

**A**: æ ¹æ®æ•°æ®æ›´æ–°é¢‘ç‡ï¼š

```
- å®æ—¶æ•°æ®: 60-300 ç§’
- å‡†å®æ—¶æ•°æ®: 300-600 ç§’
- é™æ€æ•°æ®: 3600+ ç§’
```

### Q5: å¦‚ä½•å¤„ç†ç¼“å­˜ç©¿é€ï¼Ÿ

**A**: ç¼“å­˜ç©ºå€¼ï¼š

```python
async def get_user(user_id):
    cache_key = cache._generate_key("user", id=user_id)
    
    # æ£€æŸ¥ç¼“å­˜
    cached = await cache.get(cache_key)
    if cached is not None:
        return cached if cached != "NULL" else None
    
    # æŸ¥è¯¢æ•°æ®åº“
    user = await adapter.get_one(user_id)
    
    # ç¼“å­˜ç»“æœï¼ˆåŒ…æ‹¬ç©ºå€¼ï¼‰
    await cache.set(cache_key, user or "NULL", ttl=300)
    return user
```

---

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### å…³é”®æŒ‡æ ‡

```python
# ç¼“å­˜å‘½ä¸­ç‡
hit_rate = hits / (hits + misses)

# ç¼“å­˜å®¹é‡ä½¿ç”¨ç‡
usage_rate = cache_size / max_cache_size

# å¹³å‡å“åº”æ—¶é—´
avg_response_time = total_time / request_count

# æ•°æ®åº“æŸ¥è¯¢æ•°
query_count = total_queries
```

### ç›‘æ§å·¥å…·é›†æˆ

```python
import time
from fastapi_easy.core.cache import get_query_cache

class PerformanceMonitor:
    def __init__(self):
        self.cache = get_query_cache()
        self.metrics = {
            "cache_hits": 0,
            "cache_misses": 0,
            "query_time": 0,
        }
    
    async def track_query(self, query_func):
        start = time.time()
        result = await query_func()
        elapsed = time.time() - start
        
        self.metrics["query_time"] += elapsed
        return result
    
    def get_report(self):
        hit_rate = self.metrics["cache_hits"] / (
            self.metrics["cache_hits"] + self.metrics["cache_misses"]
        ) * 100
        
        return {
            "cache_hit_rate": f"{hit_rate:.1f}%",
            "avg_query_time": f"{self.metrics['query_time']:.2f}ms",
            "cache_stats": self.cache.get_stats(),
        }
```

---

## ğŸ¯ æ€»ç»“

æ€§èƒ½ä¼˜åŒ–çš„ä¸‰ä¸ªé˜¶æ®µï¼š

| é˜¶æ®µ | ä¼˜åŒ–æ–¹å¼ | æ€§èƒ½æå‡ | å·¥ä½œé‡ |
|------|---------|---------|--------|
| åŸºç¡€ | ç¼“å­˜ã€æ‰¹é‡ã€è¿æ¥æ±  | 3-5 å€ | 1-2 å‘¨ |
| é«˜çº§ | å¤šå±‚ç¼“å­˜ã€å¼‚æ­¥ä¼˜åŒ– | 5-10 å€ | 2-3 å‘¨ |
| ç”Ÿäº§ | PostgreSQLã€ç´¢å¼•ã€åˆ†å¸ƒå¼ | 10-50 å€ | 3-4 å‘¨ |

**å»ºè®®**: ä»åŸºç¡€ä¼˜åŒ–å¼€å§‹ï¼Œé€æ­¥æ¨è¿›åˆ°é«˜çº§ä¼˜åŒ–ï¼Œæœ€åæ ¹æ®éœ€è¦è¿›è¡Œç”Ÿäº§ä¼˜åŒ–ã€‚

---

**æ€§èƒ½ä¼˜åŒ–æŒ‡å—å®Œæˆï¼** ğŸ‰

æŒ‰ç…§æœ¬æŒ‡å—å®æ–½ï¼Œå¯ä»¥æ˜¾è‘—æå‡ FastAPI-Easy åº”ç”¨çš„æ€§èƒ½ã€‚
