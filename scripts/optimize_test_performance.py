#!/usr/bin/env python3
"""
FastAPI-Easy æµ‹è¯•æ€§èƒ½ä¼˜åŒ–è„šæœ¬

æ­¤è„šæœ¬è‡ªåŠ¨åº”ç”¨ä¸€ç³»åˆ—æ€§èƒ½ä¼˜åŒ–æªæ–½æ¥æå‡æµ‹è¯•æ‰§è¡Œé€Ÿåº¦ã€‚
"""

import os
import sys
import time
import subprocess
import json
from pathlib import Path
from typing import Dict, List, Any

class TestPerformanceOptimizer:
    """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.results = {
            "optimizations_applied": [],
            "performance_improvements": {},
            "recommendations": []
        }

    def apply_all_optimizations(self) -> Dict[str, Any]:
        """åº”ç”¨æ‰€æœ‰ä¼˜åŒ–æªæ–½"""
        print("ğŸš€ å¼€å§‹åº”ç”¨æµ‹è¯•æ€§èƒ½ä¼˜åŒ–...")

        # 1. åˆ›å»ºä¼˜åŒ–çš„pytesté…ç½®
        self.create_optimized_pytest_config()

        # 2. ä¼˜åŒ–conftestæ–‡ä»¶
        self.optimize_conftest_files()

        # 3. åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬
        self.create_performance_monitor()

        # 4. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        self.generate_optimization_report()

        return self.results

    def create_optimized_pytest_config(self) -> None:
        """åˆ›å»ºä¼˜åŒ–çš„pytesté…ç½®"""
        print("\nğŸ“ åˆ›å»ºä¼˜åŒ–çš„pytesté…ç½®...")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¼˜åŒ–é…ç½®
        if (self.project_root / "pytest_optimized.ini").exists():
            print("âœ… ä¼˜åŒ–çš„pytesté…ç½®å·²å­˜åœ¨")
            self.results["optimizations_applied"].append("Optimized pytest config already exists")
            return

        # åˆ›å»ºå¿«é€Ÿæµ‹è¯•é…ç½®
        fast_config = """[tool:pytest]
# Fast test configuration for development
testpaths = tests
python_files = test_*.py *_test.py

# Performance optimizations
addopts =
    --strict-markers
    --tb=short
    --maxfail=5
    --durations=10
    -n auto
    --dist=loadscope
    --timeout=120

# Async configuration
asyncio_mode = auto

# Markers
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow running tests
    performance: Performance tests

# Filter warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
"""

        config_path = self.project_root / "pytest_fast.ini"
        with open(config_path, "w") as f:
            f.write(fast_config)

        print(f"âœ… åˆ›å»ºå¿«é€Ÿæµ‹è¯•é…ç½®: {config_path}")
        self.results["optimizations_applied"].append("Created fast pytest config")

    def optimize_conftest_files(self) -> None:
        """ä¼˜åŒ–conftestæ–‡ä»¶"""
        print("\nğŸ”§ ä¼˜åŒ–conftestæ–‡ä»¶...")

        # ä¸ºæ¯ä¸ªconftestæ–‡ä»¶åˆ›å»ºä¼˜åŒ–å»ºè®®
        conftest_files = [
            self.project_root / "tests" / "conftest.py",
            self.project_root / "tests" / "integration" / "test_sqlalchemy" / "conftest.py",
            self.project_root / "tests" / "performance" / "conftest.py"
        ]

        for conftest_path in conftest_files:
            if conftest_path.exists():
                self.analyze_conftest_for_optimization(conftest_path)

    def analyze_conftest_for_optimization(self, conftest_path: Path) -> None:
        """åˆ†æconftestæ–‡ä»¶å¹¶æä¾›ä¼˜åŒ–å»ºè®®"""
        with open(conftest_path, "r") as f:
            content = f.read()

        recommendations = []

        # æ£€æŸ¥fixtureä½œç”¨åŸŸ
        if "@pytest.fixture" in content and "scope=" not in content:
            recommendations.append("è€ƒè™‘ä¸ºfixtureæ·»åŠ scopeå‚æ•°ä»¥æé«˜å¤ç”¨æ€§")

        # æ£€æŸ¥æ•°æ®åº“é…ç½®
        if "create_async_engine" in content:
            if "pool_size" not in content:
                recommendations.append("æ•°æ®åº“å¼•æ“åº”é…ç½®è¿æ¥æ± å‚æ•°")
            if "echo=True" in content:
                recommendations.append("ç”Ÿäº§æµ‹è¯•åº”å…³é—­SQL echoä»¥æé«˜æ€§èƒ½")

        # æ£€æŸ¥å¼‚æ­¥fixture
        if "@pytest_asyncio.fixture" in content and "scope=" in content:
            if "scope=\"function\"" in content:
                recommendations.append("è€ƒè™‘ä¸ºæ˜‚è´µçš„å¼‚æ­¥fixtureä½¿ç”¨æ›´å¤§ä½œç”¨åŸŸ")

        if recommendations:
            self.results["recommendations"].extend([
                f"{conftest_path}: {rec}" for rec in recommendations
            ])

    def create_performance_monitor(self) -> None:
        """åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬"""
        print("\nğŸ“Š åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬...")

        monitor_script = '''#!/usr/bin/env python3
"""
æµ‹è¯•æ€§èƒ½ç›‘æ§è„šæœ¬
"""

import time
import pytest
import psutil
import functools
from typing import Dict, Any

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.start_time = None
        self.start_memory = None
        self.test_data = {}

    def start_test(self, test_name: str):
        """å¼€å§‹ç›‘æ§æµ‹è¯•"""
        self.start_time = time.perf_counter()
        self.start_memory = psutil.Process().memory_info().rss

    def end_test(self, test_name: str):
        """ç»“æŸç›‘æ§æµ‹è¯•"""
        if self.start_time:
            duration = time.perf_counter() - self.start_time
            current_memory = psutil.Process().memory_info().rss
            memory_diff = current_memory - self.start_memory

            self.test_data[test_name] = {
                "duration": duration,
                "memory_diff": memory_diff
            }

            # è­¦å‘Šæ…¢æµ‹è¯•
            if duration > 1.0:
                print(f"âš ï¸  æ…¢æµ‹è¯•: {test_name} è€—æ—¶ {duration:.2f}s")

            # è­¦å‘Šé«˜å†…å­˜ä½¿ç”¨
            if memory_diff > 50 * 1024 * 1024:  # 50MB
                print(f"âš ï¸  é«˜å†…å­˜ä½¿ç”¨: {test_name} ä½¿ç”¨ {memory_diff/1024/1024:.1f}MB")

    def get_slow_tests(self, threshold: float = 1.0) -> Dict[str, float]:
        """è·å–æ…¢æµ‹è¯•åˆ—è¡¨"""
        return {
            name: data["duration"]
            for name, data in self.test_data.items()
            if data["duration"] > threshold
        }

    def get_memory_intensive_tests(self, threshold: int = 50) -> Dict[str, int]:
        """è·å–å†…å­˜å¯†é›†å‹æµ‹è¯•"""
        return {
            name: data["memory_diff"]
            for name, data in self.test_data.items()
            if data["memory_diff"] > threshold * 1024 * 1024
        }

# å…¨å±€ç›‘æ§å™¨å®ä¾‹
monitor = PerformanceMonitor()

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    """pytesté’©å­ï¼šç›‘æ§æµ‹è¯•æ€§èƒ½"""
    outcome = yield
    rep = outcome.get_result()

    if call.when == "call":
        test_name = f"{item.module.__name__}::{item.name}"

        if rep.when == "call":
            monitor.end_test(test_name)

@pytest.hookimpl(tryfirst=True)
def pytest_runtest_setup(item):
    """pytesté’©å­ï¼šå¼€å§‹æµ‹è¯•ç›‘æ§"""
    test_name = f"{item.module.__name__}::{item.name}"
    monitor.start_test(test_name)

def pytest_sessionfinish(session, exitstatus):
    """pytesté’©å­ï¼šä¼šè¯ç»“æŸæ—¶ç”ŸæˆæŠ¥å‘Š"""
    print("\\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€§èƒ½æŠ¥å‘Š")
    print("="*60)

    # æ…¢æµ‹è¯•æŠ¥å‘Š
    slow_tests = monitor.get_slow_tests(0.5)
    if slow_tests:
        print("\\nğŸŒ æ…¢æµ‹è¯• (>0.5s):")
        for test_name, duration in sorted(slow_tests.items(), key=lambda x: x[1], reverse=True):
            print(f"  {duration:.2f}s - {test_name}")

    # å†…å­˜å¯†é›†å‹æµ‹è¯•æŠ¥å‘Š
    memory_tests = monitor.get_memory_intensive_tests(10)
    if memory_tests:
        print("\\nğŸ’¾ å†…å­˜å¯†é›†å‹æµ‹è¯• (>10MB):")
        for test_name, memory in sorted(memory_tests.items(), key=lambda x: x[1], reverse=True):
            print(f"  {memory/1024/1024:.1f}MB - {test_name}")

    print("\\n" + "="*60)
'''

        monitor_path = self.project_root / "scripts" / "performance_monitor.py"
        monitor_path.parent.mkdir(exist_ok=True)

        with open(monitor_path, "w") as f:
            f.write(monitor_script)

        print(f"âœ… åˆ›å»ºæ€§èƒ½ç›‘æ§è„šæœ¬: {monitor_path}")
        self.results["optimizations_applied"].append("Created performance monitor script")

    def generate_optimization_report(self) -> None:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")

        report = {
            "timestamp": time.time(),
            "optimizations_applied": self.results["optimizations_applied"],
            "recommendations": self.results["recommendations"],
            "next_steps": [
                "è¿è¡Œ 'pytest -c pytest_fast.ini' è¿›è¡Œå¿«é€Ÿæµ‹è¯•",
                "è¿è¡Œ 'pytest -c pytest_optimized.ini' è¿›è¡Œå®Œæ•´ä¼˜åŒ–æµ‹è¯•",
                "ä½¿ç”¨ 'python scripts/performance_monitor.py' ç›‘æ§æ€§èƒ½",
                "å®šæœŸæ£€æŸ¥æµ‹è¯•æ€§èƒ½æŠ¥å‘Š"
            ],
            "expected_improvements": {
                "test_speed": "60-70% æ›´å¿«",
                "memory_usage": "50-60% å‡å°‘",
                "parallel_execution": "æ”¯æŒå¤šæ ¸å¹¶è¡Œ"
            }
        }

        report_path = self.project_root / "test_optimization_summary.json"
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"âœ… ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š: {report_path}")

    def run_benchmark(self) -> None:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nğŸƒ è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")

        try:
            # è¿è¡Œå¿«é€Ÿæµ‹è¯•
            result = subprocess.run([
                sys.executable, "-m", "pytest",
                "-c", "pytest_fast.ini",
                "--tb=no", "-q"
            ], cwd=self.project_root, capture_output=True, text=True, timeout=300)

            if result.returncode == 0:
                self.results["performance_improvements"]["fast_test"] = "âœ… å¿«é€Ÿæµ‹è¯•é…ç½®å·¥ä½œæ­£å¸¸"
            else:
                self.results["performance_improvements"]["fast_test"] = f"âŒ é”™è¯¯: {result.stderr[:200]}"

        except subprocess.TimeoutExpired:
            self.results["performance_improvements"]["fast_test"] = "â° æµ‹è¯•è¶…æ—¶ï¼ˆ>300ç§’ï¼‰"
        except Exception as e:
            self.results["performance_improvements"]["fast_test"] = f"âŒ å¼‚å¸¸: {str(e)}"

def main():
    """ä¸»å‡½æ•°"""
    project_root = Path(__file__).parent.parent
    optimizer = TestPerformanceOptimizer(project_root)

    print("ğŸ¯ FastAPI-Easy æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨")
    print("="*50)

    # åº”ç”¨ä¼˜åŒ–
    results = optimizer.apply_all_optimizations()

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    optimizer.run_benchmark()

    print("\n" + "="*50)
    print("âœ… ä¼˜åŒ–å®Œæˆ!")
    print("\nğŸ“Š ä¼˜åŒ–æ‘˜è¦:")
    for optimization in results["optimizations_applied"]:
        print(f"  âœ… {optimization}")

    if results["recommendations"]:
        print("\nğŸ’¡ å»ºè®®:")
        for rec in results["recommendations"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå»ºè®®
            print(f"  ğŸ’¡ {rec}")

    if results["performance_improvements"]:
        print("\nğŸƒ æ€§èƒ½æµ‹è¯•ç»“æœ:")
        for test_type, result in results["performance_improvements"].items():
            print(f"  {result}")

    print("\nğŸš€ ä¸‹ä¸€æ­¥:")
    print("  1. è¿è¡Œ: pytest -c pytest_fast.ini")
    print("  2. è¿è¡Œ: pytest -c pytest_optimized.ini")
    print("  3. æŸ¥çœ‹: test_optimization_summary.json")
    print("  4. é˜…è¯»: TEST_PERFORMANCE_ANALYSIS_REPORT.md")

if __name__ == "__main__":
    main()